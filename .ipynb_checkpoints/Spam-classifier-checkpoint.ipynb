{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path = os.getcwd()\n",
    "import email\n",
    "import email.policy as policy\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "from urlextract import URLExtract\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multipart_to_text(mail):\n",
    "    for part in mail.get_payload():\n",
    "        if part.get_content_type() in [\"text/plain\", \"text/html\"]:\n",
    "            return BeautifulSoup(part.get_content()).get_text().replace(\"\\n\\n\", \" \")\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mails(dir_name):\n",
    "    global dir_path\n",
    "    emails = []\n",
    "    i = 1\n",
    "    for filename in os.listdir(dir_name):\n",
    "        with open(\"{}\\\\{}\\\\{}\".format(dir_path, dir_name, filename)) as f:\n",
    "            try:\n",
    "                mail = email.parser.Parser(policy = policy.default).parse(f)\n",
    "                if \"multipart\" in mail.get_content_type():\n",
    "                    multipart_str = multipart_to_text(mail)\n",
    "                    if multipart_str:\n",
    "                        emails.append(multipart_str)\n",
    "                else:\n",
    "                    emails.append(BeautifulSoup(mail.get_content()).get_text().replace(\"\\n\\n\", \" \"))\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_mails = read_mails(\"Spam\\\\spam\")\n",
    "spam_2_mails = read_mails(\"Spam\\\\spam_2\")\n",
    "ham_mails = read_mails(\"Ham\\\\easy_ham\")\n",
    "ham_2_mails = read_mails(\"Ham\\\\easy_ham_2\")\n",
    "ham_hard_mails = read_mails(\"Ham\\\\hard_ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FC Sporadic  Hi,  Check out my new site,\n",
      "InternalMemos.com \t\tOver 980 internal memos, updated daily.\n",
      "\t\t\n",
      "\t\tRecent highlights include:  \n",
      "\t\t\n",
      "A&P: Salary, hiring freeze\n",
      "$Exodus: Rumors, rumors, rumors\n",
      "MSNBC: Sniper, parties, other news\n",
      "$Ebay: San Jose office moving?\n",
      "$EDS: Incentive & bonus changes\n",
      "Adobe: Workforce Realignment/Reduction\n",
      "$Lycos: Frank Weller resignation\n",
      "AOL: Past, present, and future\n",
      "TenFold: Reorganization\n",
      "Ebay: Meg addresses allegations\n",
      "$ Verisign: Third-party phase-out\n",
      "$ Ericsson: Market unstable, moving on More internal memos...   Let me first welcome all the new people to the Sporadic -- there were a bunch of people who signed up a while ago but never got added until today, I think the problems have been fixed.  There are currently over 250,000 subscribers to this \n",
      "list (dayam!).  Of those, I think there are around 5 people who actually read it. So anyway, raise your hand if you're older than 30 and have never been married.   Unless I meet the girl of my dreams tomorrow (which is unlikely as I don't plan on leaving my apartment tomorrow, much less wearing pants anytime in the near future), looks like I'm headed in that direction. I never think about it but it seems I'm supposed to.  I definitely don't want to be one of those guys who's a bachelor for life, and I eventually want kids, I think that's cool!  But today, October 30, 2002, is my 27th birthday. To celebrate, I bought myself some Rogain, tooth whitener, and a gym membership.  My detergent makes the skin on my back hurt and I still have zits.  Oh hey I bought a new guitar and played dress up, that was fun. It's 4:00 AM and I'm eating expired ham.  Somehow I'm able to write a newsletter to 250,000 people, yet my birthday plans involve sitting home alone teaching myself Journey songs on the guitar.  then Blockbuster and Chinese delivery.  \n",
      "Woohoo!   Anyway, thanks for being there.  Rock on,\n",
      "pud ps- man, this new Sporadic layout is a little too cheery, don't you think?  blah, i'll fix it later.\n",
      "pps- if you need web hosting or are a hosting provider, check out my new site Bizient.com \n",
      "Here are some of today's newest fucks.  As usual, more new ones on the site daily.\n",
      "http://www.fuckedcompany.com/ Booyah\n",
      "Wowwwww..... this internal memo sent to Clifford \n",
      "Chance LLP employees lists a ton of \"problem areas\", complaints and comments... Highlights include, \"If the assigning system isn't corrupt, ask yourself: why aren't attractive female associates ever out of work?\" and \"Not only is he \n",
      "dishonest, but he behaves completely inappropriately with female associates working under him\".. and so on.\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\tWhen: 10/30/2002\n",
      "\t\t\t\t\t\t\tCompany: Clifford Chance LLP\n",
      "\t\t\t\t\t\t\tPoints: 135  Grrr\n",
      "Vignette to Acquire Epicentric.  Here's the news, and of \n",
      "course here's the internal memo that was sent to employees.  From one FC reader and pissed-off employee who worked hard for his options, \"Here's \n",
      "my favorite one.  Q. What will happen to my Epicentric employee stock options? A.  Once the purchase of Epicentric is complete, Epicentric stock options will be cancelled.\"\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\tWhen: 10/30/2002\n",
      "\t\t\t\t\t\t\tCompany: Epicentric\n",
      "\t\t\t\t\t\t\tPoints: 185   Limb\n",
      "Bausch and Lomb CEO denied $1.1 million bonus \n",
      "for falsely claiming to have completed a masters degree.  What a fuckbasket cock-socket he is.\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\tWhen: 10/30/2002\n",
      "\t\t\t\t\t\t\tCompany: Bausch & Lomb \n",
      "\t\t\t\t\t\t\tPoints: 115   Cleanup, isle four\n",
      "Salary and hiring freeze at A&P supermarkets, here's the internal memo sent to employees.\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\tWhen: 10/30/2002\n",
      "\t\t\t\t\t\t\tCompany: A&P\n",
      "\t\t\t\t\t\t\tPoints: 120   CyaNow\n",
      "Okee I reported yesterday that CDNow was set to layoff 60 people... well rumor has it the \n",
      "axe just fell and the *entire* programming staff was let go.  Whether this means the company is shutting down or they're just being replaced by cheaper consultants remains to be seen...\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\tWhen: 10/30/2002\n",
      "\t\t\t\t\t\t\tCompany: CdNow\n",
      "\t\t\t\t\t\t\tPoints: 169   Sore\n",
      "Rumor has it Kestrel Solutions filed Chapter 11 today.  Anyone need a sweet deal \n",
      "on a bit-rate independent multiplexer?  uhh.. didnt think so.\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\tWhen: 10/30/2002\n",
      "\t\t\t\t\t\t\tCompany: Kestrel Solutions\n",
      "\t\t\t\t\t\t\tPoints: 165   XXX\n",
      "Rumor has it massive layoffs in the pipline at Xerox.  Word is high-ranking managers are \n",
      "having assignments removed, work load reduced, etc... Sometime mid-next week is said to be d-day. \n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\tWhen: 10/30/2002\n",
      "\t\t\t\t\t\t\tCompany: Xerox \n",
      "\t\t\t\t\t\t\tPoints: 143 Copyright 2002.  too tired to think of anything creative.  thanks for reading tho :) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ham_hard_mails[247])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mails in spam = 483\n",
      "Number of mails in spam_2 = 1337\n",
      "Number of mails in easy_ham = 2499\n",
      "Number of mails in easy_ham_2 = 1398\n",
      "Number of mails in hard_ham = 248\n",
      "\n",
      "Total number of spam mails = 1820\n",
      "Total number of ham mails = 4145\n",
      "\n",
      "Percentage of spam mails = 0.30511316010058676\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mails in spam =\", len(spam_mails))\n",
    "print(\"Number of mails in spam_2 =\", len(spam_2_mails))\n",
    "print(\"Number of mails in easy_ham =\", len(ham_mails))\n",
    "print(\"Number of mails in easy_ham_2 =\", len(ham_2_mails))\n",
    "print(\"Number of mails in hard_ham =\", len(ham_hard_mails))\n",
    "print()\n",
    "print(\"Total number of spam mails =\", len(spam_mails) + len(spam_2_mails))\n",
    "print(\"Total number of ham mails =\", len(ham_mails) + len(ham_2_mails) + len(ham_hard_mails))\n",
    "print()\n",
    "print(\"Percentage of spam mails =\", (len(spam_mails) + len(spam_2_mails)) / (len(spam_mails) + len(spam_2_mails) + len(ham_mails) + len(ham_2_mails) + len(ham_hard_mails)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_email(email, mail_type):\n",
    "    #Replace all URLs with \"urllink\"\n",
    "    extractor = URLExtract()\n",
    "    for i in extractor.find_urls(email):\n",
    "        email = email.replace(i, \"urllink\")\n",
    "    \n",
    "    #Replace special characters with spaces\n",
    "    email = email.replace(\"e-mail\", \"email\")\n",
    "    email = re.sub(\"[!@#$%^&*()\\[\\]{};:,./<>?\\|`~\\-=_+]\", \" \", email)\n",
    "    \n",
    "    #Replace all numbers with \"numberpresent\"\n",
    "    email = re.sub(\"[0-9]+\", \" numberpresent \", email)\n",
    "    \n",
    "    #Convert all characters to lowercase\n",
    "    email = email.lower()\n",
    "    \n",
    "    #Tokenize the email, remove the stopwords, and stem the words\n",
    "    stemmer = PorterStemmer()\n",
    "    unprocessed_words = [w for w in word_tokenize(email) if w not in stopwords.words('english')]\n",
    "    words = [stemmer.stem(w) for w in unprocessed_words]\n",
    "    \n",
    "    return {\"unprocessed_words\": unprocessed_words, \"words\": words, \"type\": mail_type * 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(data_list, train_percentage):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for current_data in data_list:\n",
    "        mail_type = current_data[\"type\"] == \"spam\"\n",
    "        split_size = len(current_data[\"data\"]) * train_percentage // 100\n",
    "        train_set += [process_email(mail, mail_type) for mail in (current_data[\"data\"][0:split_size])]\n",
    "        test_set += [process_email(mail, mail_type) for mail in (current_data[\"data\"][split_size:])]\n",
    "    \n",
    "    unprocessed_ham_words = [mail[\"unprocessed_words\"] for mail in train_set if mail[\"type\"] == 0]\n",
    "    unprocessed_ham_words += [mail[\"unprocessed_words\"] for mail in test_set if mail[\"type\"] == 0]\n",
    "    unprocessed_spam_words = [mail[\"unprocessed_words\"] for mail in train_set if mail[\"type\"] == 1]\n",
    "    unprocessed_spam_words += [mail[\"unprocessed_words\"] for mail in test_set if mail[\"type\"] == 1]\n",
    "    \n",
    "    \n",
    "    return [train_set, test_set, unprocessed_ham_words, unprocessed_spam_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, unprocessed_ham_words, unprocessed_spam_words = create_datasets([{\"data\": spam_mails, \"type\": \"spam\"},\\\n",
    "                                                                                {\"data\": spam_2_mails, \"type\": \"spam\"},\\\n",
    "                                                                                {\"data\": ham_mails, \"type\": \"ham\"},\\\n",
    "                                                                                {\"data\": ham_2_mails, \"type\": \"ham\"},\\\n",
    "                                                                                {\"data\": ham_hard_mails, \"type\": \"ham\"}], 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_spam_words = sum(unprocessed_spam_words, [])\n",
    "unprocessed_ham_words = sum(unprocessed_ham_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(unprocessed_spam_words)\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
